\documentclass[11pt]{amsart}

\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{fullpage}
\usepackage{tikz-cd}
% \usepackage{euscript}
% \makeatletter
% \nopagenumbers
\usepackage{verbatim}
\usepackage{color}
\usepackage{hyperref}

\usepackage{fullpage,tikz,float}
%\usepackage{times} %, mathtime}

\textheight=600pt %574pt
\textwidth=480pt %432pt
\oddsidemargin=15pt %18.88pt
\evensidemargin=18.88pt
\topmargin=10pt %14.21pt

\parskip=1pt %2pt

% define theorem environments
\newtheorem{theorem}{Theorem}    %[section]
%\def\thetheorem{\unskip}
\newtheorem{proposition}[theorem]{Proposition}
%\def\theproposition{\unskip}
\newtheorem{conjecture}[theorem]{Conjecture}
\def\theconjecture{\unskip}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{sublemma}[theorem]{Sublemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{observation}[theorem]{Observation}
%\def\thelemma{\unskip}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\def\thedefinition{\unskip}
\newtheorem{notation}[definition]{Notation}
\newtheorem{remark}[definition]{Remark}
% \def\theremark{\unskip}
\newtheorem{question}[definition]{Question}
\newtheorem{questions}[definition]{Questions}
%\def\thequestion{\unskip}
\newtheorem{example}[definition]{Example}
%\def\theexample{\unskip}
\newtheorem{problem}[definition]{Problem}
\newtheorem{exercise}[definition]{Exercise}

\numberwithin{theorem}{section}
\numberwithin{definition}{section}
\numberwithin{equation}{section}

\def\reals{{\mathbb R}}
\def\torus{{\mathbb T}}
\def\integers{{\mathbb Z}}
\def\rationals{{\mathbb Q}}
\def\naturals{{\mathbb N}}
\def\complex{{\mathbb C}\/}
\def\distance{\operatorname{distance}\,}
\def\support{\operatorname{support}\,}
\def\dist{\operatorname{dist}\,}
\def\Span{\operatorname{span}\,}
\def\degree{\operatorname{degree}\,}
\def\kernel{\operatorname{kernel}\,}
\def\dim{\operatorname{dim}\,}
\def\codim{\operatorname{codim}}
\def\trace{\operatorname{trace\,}}
\def\dimension{\operatorname{dimension}\,}
\def\codimension{\operatorname{codimension}\,}
\def\nullspace{\scriptk}
\def\kernel{\operatorname{Ker}}
\def\p{\partial}
\def\Re{\operatorname{Re\,} }
\def\Im{\operatorname{Im\,} }
\def\ov{\overline}
\def\eps{\varepsilon}
\def\lt{L^2}
\def\curl{\operatorname{curl}}
\def\divergence{\operatorname{div}}
\newcommand{\norm}[1]{ \|  #1 \|}
\def\expect{\mathbb E}
\def\bull{$\bullet$\ }
\def\det{\operatorname{det}}
\def\Det{\operatorname{Det}}
\def\rank{\mathbf r}
\def\diameter{\operatorname{diameter}}

\def\t2{\tfrac12}

\newcommand{\abr}[1]{ \langle  #1 \rangle}

\def\newbull{\medskip\noindent $\bullet$\ }
\def\field{{\mathbb F}}
\def\cc{C_c}



% \renewcommand\forall{\ \forall\,}

% \newcommand{\Norm}[1]{ \left\|  #1 \right\| }
\newcommand{\Norm}[1]{ \Big\|  #1 \Big\| }
\newcommand{\set}[1]{ \left\{ #1 \right\} }
%\newcommand{\ifof}{\Leftrightarrow}
\def\one{{\mathbf 1}}
\newcommand{\modulo}[2]{[#1]_{#2}}

\def\bd{\operatorname{bd}\,}
\def\cl{\text{cl}}
\def\nobull{\noindent$\bullet$\ }

\def\scriptf{{\mathcal F}}
\def\scriptq{{\mathcal Q}}
\def\scriptg{{\mathcal G}}
\def\scriptm{{\mathcal M}}
\def\scriptb{{\mathcal B}}
\def\scriptc{{\mathcal C}}
\def\scriptt{{\mathcal T}}
\def\scripti{{\mathcal I}}
\def\scripte{{\mathcal E}}
\def\scriptv{{\mathcal V}}
\def\scriptw{{\mathcal W}}
\def\scriptu{{\mathcal U}}
\def\scriptS{{\mathcal S}}
\def\scripta{{\mathcal A}}
\def\scriptr{{\mathcal R}}
\def\scripto{{\mathcal O}}
\def\scripth{{\mathcal H}}
\def\scriptd{{\mathcal D}}
\def\scriptl{{\mathcal L}}
\def\scriptn{{\mathcal N}}
\def\scriptp{{\mathcal P}}
\def\scriptk{{\mathcal K}}
\def\scriptP{{\mathcal P}}
\def\scriptj{{\mathcal J}}
\def\scriptz{{\mathcal Z}}
\def\scripts{{\mathcal S}}
\def\scriptx{{\mathcal X}}
\def\scripty{{\mathcal Y}}
\def\frakv{{\mathfrak V}}
\def\frakG{{\mathfrak G}}
\def\aff{\operatorname{Aff}}
\def\frakB{{\mathfrak B}}
\def\frakC{{\mathfrak C}}

\def\symdif{\,\Delta\,}
\def\mustar{\mu^*}
\def\muplus{\mu^+}

\def\soln{\noindent {\bf Solution.}\ }


%\pagestyle{empty}
%\setlength{\parindent}{0pt}

\begin{document}

\begin{center}{\bf Math 110 --- Homework 7 --- UCB, Summer 2017 --- William Guss}
\end{center}

\medskip \noindent {\textbf{(7.1) } Let $p,q \in C^\infty([-1,1])$ be real-valued with $p(-1) = 0 = p(1)$, and define $T: C^\infty([-1,1]) \to C^\infty([-1,1])$ by
\begin{equation}
	[T(f)](x) := -\frac{d}{dx}\left[p(x) \frac{d}{dx} f(x)\right] - q(x) f(x).
\end{equation}
Show that the eigenvectors of $T$ with distinct eigenvalues are orthogonal.
\begin{proof}
	For distinct eigenvalues $\lambda_1$ and $\lambda_2$ take $f_1, f_2$ to be some respective eigenvectors. Recall that $\langle f_1| f_2 \rangle = 0 $ iff $\alpha \langle f_1| f_2 \rangle = 0$ for $\alpha \neq 0$. Then
	\begin{equation*}
		\begin{aligned}
			\overline {f_1(x)} f_2(x) \lambda_2 - f_2(x) \overline{f_1(x) \lambda_1} &= -\overline {f_1(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} f_2(x)\right] - \overline{f_1(x)} q(x) f_2(x) \\
			&\;\;\;\; + {f_2(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} \overline{f_1(x)}\right] + \overline{f_1(x)} q(x) f_2(x) \\
			&=  -\overline {f_1(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} f_2(x)\right] + {f_2(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} \overline{f_1(x)}\right]
		\end{aligned}
	\end{equation*}
	Applying the Liebiniz rule we get
	\begin{equation*}
	\begin{aligned}
		(\lambda_2 - \lambda_1)\overline {f_1(x)} f_2(x) 
			&=  -\overline {f_1(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} f_2(x)\right] 
			   + {f_2(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} \overline{f_1(x)}\right] \\
			&\;\;\;\; + \left[p(x) \frac{df_2}{dx} \frac{d\overline{f_1}}{dx} -  p(x) \frac{df_2}{dx} \frac{d\overline{f_1}}{dx}  \right]\\
			&=  -\overline {f_1(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} f_2(x)\right] 
			    -  p(x) \frac{df_2}{dx} \frac{d\overline{f_1}}{dx}  \\
			&\;\;\;\;+ {f_2(x)} \frac{d}{dx}\left[p(x) \frac{d}{dx} \overline{f_1(x)}\right] + p(x) \frac{df_2}{dx} \frac{d\overline{f_1}}{dx}\\
			&= -\frac{d}{dx} p(x) \left[\overline{f_1(x)} \frac{df_2}{dx}\right] + \frac{d}{dx} p(x) \left[f_2(x) \frac{d\overline{f_1}}{dx}\right] \\
			&= \frac{d}{dx} \left[p(x) \left(f_2(x) \frac{df_1}{dx} - \overline{f_1(x)}\frac{df_2}{dx}\right)\right].
	\end{aligned}
	\end{equation*}
	We now can compute the inner product via integration, and yielf
	\begin{equation*}
	\begin{aligned}
		(\lambda_2 - \lambda_1) \langle f_1 |  f_2\rangle &= (\lambda_2 - \lambda_1) \int_{-1}^1 \overline {f_1(x)} f_2(x)\; dx  \\
		&= \left[p(x) \left(f_2(x) \frac{df_1}{dx} - \overline{f_1(x)}\frac{df_2}{dx}\right)\right]_{-1}^1 \\
		&= p(1) \left[f_2(x) \frac{df_1}{dx} - \overline{f_1(x)}\frac{df_2}{dx}\right]_{x = 1} - p(0) \left[f_2(x) \frac{df_1}{dx} - \overline{f_1(x)}\frac{df_2}{dx}\right]_{x = 0}\\
		&= 0 - 0
	\end{aligned}
	\end{equation*}
	Since $\lambda_1$ and $\lambda_2$ are distinct, we have $\langle f_1 |  f_2\rangle = 0$ and therefore the eigenvectors of disctinct eigenvalues of $T$ are orthogonal. 
\end{proof}

\medskip \noindent {\textbf{(7.2) } Show that $\{\frac{1}{\sqrt{2 \pi } e^{inx}}\}$ is orthonormal in $C^\infty((-\pi, \pi)).$

\begin{proof}
	Take $n \neq m \in \mathbb{Z}$ then
	\begin{equation*}
		\begin{aligned}
			\langle f_n, f_m \rangle &= \frac{1}{2\pi}\int_{-\pi}^\pi e^{-inx} e^{imx}\ dx	 \\
			&= \frac{1}{2\pi}\int_{-\pi}^\pi e^{(m-n)ix}\ dx	
			= \frac{1}{2\pi(m-n)i} e^{(m-n)ix}\Big|_{-\pi}^\pi	\\
			&= \frac{1}{2\pi ki} \left[e^{ik\pi} - e^{-ik{\pi}}\right] = \frac{1}{2\pi ki} \left[e^{ik\pi} - e^{ik{\pi}}\right] = 0
		\end{aligned}
	\end{equation*}
	since $k:= m-n$ is an non-zero integer, and whence the imaginary part of $e^{ikx}$ is zero. Next, fix $m$ and then
	\begin{equation*}
		\|f_m\|^2 = \langle f_m, f_m \rangle =  \frac{1}{2\pi}\int_{-\pi}^\pi e^{-imx} e^{imx} dx =  \frac{1}{2\pi}\int_{-\pi}^\pi dx = \frac{2\pi}{2\pi} = 1. 
	\end{equation*}
	Thus the basis is orthonormal.
\end{proof}


\medskip \noindent {\textbf{(7.3) } Let $V$ be a complex vector space and let $\| \cdot \|: V \to \mathbb{R}_0^+$ be a norm on $V.$ Show that $\|v\|^2 = \langle v | v \rangle$ for some inner-product $\langle \cdot | \cdot \rangle: V \times V \to \mathbb{C}$ if and only if $\|\cdot\|$ satisfies the Parallelogram Law.

\begin{proof}
	Suppose that $\|v\|^2 = \langle v | v\rangle$ for some inner product $V\times V \to \mathbb{C}$. We calculate as follows
	\begin{equation*}
	\begin{aligned}
		\| v + w\|^2 + \|v-w\|^2 &= \langle v +w, v+w\rangle + \langle v-w, v-w \rangle \\
		&= \langle v, v \rangle + \langle v, w\rangle + \langle , v\rangle + \langle w,w\rangle \\
		&\;\;\;\;\;\langle v, v \rangle - \langle v, w \rangle - \langle w,v \rangle + \langle w, w \rangle \\
		&= 2(\|v\|^2 + \|w\|^2).
	\end{aligned}
	\end{equation*}
	where $\|x\|^2 = \langle w, w \rangle$ is applied in the first step. Therefore if $V$ has an inner product, the induced norm satisifes the Parallelogram Law.

	In the other direction, suppose that $V$ has a norm which satisfies the parallelogran law. We claim that the following is an inner product whose square is the norm.
	\begin{equation*}
		\langle v | w \rangle := \frac{1}{4}\left(\|v + w\|^2 - \|v-w\|^2 - i\|v + iw\|^2 + i \|v - iw\|^2\right)
	\end{equation*}
	First, we evaluate the inner product restricted to the diagonal of $V \times V$. Take $v \in V$ then,
	\begin{equation*}
		\begin{aligned}
			\langle v| v\rangle  &=  \frac{1}{4}\left(\|v + v\|^2 - \|v-v\|^2 - i\|v + iv\|^2 + i \|v - iv\|^2\right) \\
			&= \frac{1}{4}\left(\|v + v\|^2 - i\|v + iv\|^2 + i \|v - iv\|^2\right) \\
			&= \frac{1}{4}\left(\|v + v\|^2  + i(\|v - iv\|^2 - \|v + iv\|^2 )\right) \\
			&= \frac{1}{4}\left(\|2v\|^2  + i(|1-i|^2\|v\|^2 - |1+i|^2\|v\|^2 )\right) \\
			&=  \|v\|^2.
		\end{aligned}
	\end{equation*}


	Next we check that the inner product structure is satisfied. If $\langle v | v \rangle = 0$ then $\|v\|^2 = 0$ by the foregoing algebra, and by the definition of norms, $v$ must be $0$. The other direction follows by plugging $0$ into the norm definition. Again by the definition of norm $\langle v, v\rangle = \|v\|^2$ is always non-negative.

	For the rest of the proof we will restrict our abnalysis to the real case and then extend to the imaginary case. For symmetry, let $v, w \in V$ and then
	\begin{equation*}
	\begin{aligned}
		\langle v |w\rangle = \frac{1}{4}\left(\|v + w\|^2 - \|v -w\|^2\right) \\
		 = \frac{1}{4}\left(\|v+ w\|^2 - \|-(w -v)\|^2\right) \\
		  = \frac{1}{4}\left(\|v+ w\|^2 - \|(w -v)\|^2\right)  = \langle w, v \rangle.
	\end{aligned}
	\end{equation*}

	Turning our attention to additivity, let $z \in V$ and using the parallelogram law we yield that 
	\begin{equation*}
	\begin{aligned}
		\|v + w + z\|^2 + \|v- w + z \|^2 = 2\|v +z\| + 2 \|w\| \\ 
		\|v + w + z \|^2 = 2 \|v + z\|^2 + 2\|w\|^2 - \|v -w + z\|^2 \\
		\|v + w + z \|^2 = 2 \|w + z\|^2 + 2\|w\|^2 - \|w-v + z\|^2 \\
	\end{aligned}
	\end{equation*}
	Therefore it follows using $\|y-x\| = \|x-y\|$ and the results from the foregoing application
	\begin{equation*}
	\begin{aligned}
		\|v + w + z\|^2 = \|v + z\|^2 + \|w + z\|^2 + \|v\|^2 + \|w\|^2  - \frac{\|v - w + z\|^2 + \|w- v + z\|^2}{2}. \\
		\|v + w - z\|^2 = \|v - z\|^2 + \|w - z\|^2+  \|v\|^2 + \|w\|^2  - \frac{\| w + z -v\|^2 + \| z + v - w\|^2}{2}.
	\end{aligned}
	\end{equation*}
	In the additive case,
	\begin{equation*}
		\begin{aligned}
			4\langle v + w| z \rangle &=  \|v + w+ z\|^2 - \|v + w -z\|^2 \\
			&=   \|v + z\|^2 + \|w + z\|^2 + \|v\|^2 + \|w\|^2  - \frac{\|v - w + z\|^2 + \|w- v + z\|^2}{2}\\
			&\;\;\;- \left(\|v - z\|^2 + \|w - z\|^2+  \|v\|^2 + \|w\|^2  - \frac{\| w + z -v\|^2 + \| z + v - w\|^2}{2}\right) \\
			&=   \|v + z\|^2 + \|w + z\|^2  - \|v - z\|^2 - \|w - z\|^2\\
			&=   \|v + z\|^2 - \|v - z\|^2 + \|w + z\|^2  - \|w - z\|^2 \\
			&= 4\langle v| z\rangle + 4 \langle w| z \rangle.
		\end{aligned}
	\end{equation*}

	We now will show that this inner product commutes with scalars\footnote{I've borrowed this argument from an operator theory textbook}. In particular we wish to show
	$\lambda \langle v| w\rangle = \langle \lambda v, w\rangle$ for all $\lambda \in \mathbb{R}.$  Take the case of $\lambda = -1$, then 
	\begin{equation*}
		\langle -v | w \rangle = \frac{1}{4}\left(\|-v + w\|^2 - \|- v -w\|^2\right) = \frac{1}{4}\left(-\|v + w\|^2 + \|v -w\|^2\right) = - \langle v| w \rangle.
	\end{equation*}
	Then by induction the result holds for all $\mathbb{Z}$ applying the previous result. Let $p/q=\lambda \in \mathbb{Q}$, then
	\begin{equation*}
		q \langle p/q v| w\rangle = p \langle q/q v| w\rangle =  p \langle v| w\rangle \implies  \langle p/q v| w\rangle = p/q \langle v| w\rangle
	\end{equation*}
	Thus homogeneity holds for all $\lambda \in \mathbb{Q}.$ We can extend this to all $\mathbb{R}$ by recalling that scalar multiplication, vector addition, and the norm itself are continuous functions on $V$, in which case, equality on a dense subset ($\mathbb{Q}$)  yields equality on the closure $(\mathbb{R})$. This completes the proof.

	The complex case is as follows, we need show that $ \langle iv |w \rangle  = i \langle v |w \rangle.$ It folllows as
	\begin{equation*}
	\begin{aligned}
	\langle iv |w \rangle &=  \frac{1}{4}\left(\|iv + w\|^2 - \|iv-w\|^2 - i\|iv + iw\|^2 + i \|iv - iw\|^2\right) \\
	&=  \frac{1}{4}\left(- i\|v + w\|^2 + i \|v - w\|^2 + \|iv + w\|^2 - \|iv-w\|^2 \right) \\
	&= -i \langle v| w \rangle.
	\end{aligned}
	\end{equation*}
	This completes the proof.
	\end{proof}

\medskip \noindent {\textbf{(7.4) } Let $V$ be a complex vector space, then 
	\begin{equation*}
  		\langle v | w \rangle = \frac{1}{4}\left( \|v + w\|^2 - \|v - w\|^2 + i\|v+iw\|^2 + i\|v-iw\|^2\right).
  	\end{equation*} 
  	\begin{proof}
  	 	To verify this proof, we will first compute the inner product in terms of $\|v+w\|.$ 
  	 	\begin{equation*}
  	 		\|v+w\|^2 = \langle v + w| v + w\rangle = \|v\|^2 + \|w\|^2 + Re(\langle v| w\rangle ),
  	 	\end{equation*}
  	 	where the last term comes from the sum of the inner product with its conjugate.

  	 	Now we consider the imaginary piece and yield
  	 	\begin{equation*}
  	 		\|v + iw\|^2 = \langle v + iw|v+iw\rangle = \|v\|^2 + \|w\|^2 - 2Im(\langle v | w \rangle).
  	 	\end{equation*}

  	 	Repeating the steps above with $v -w$, we get that
  	 	\begin{equation*}
  	 	\begin{aligned}
  	 	  	 		\|v -w\|^2 = \|v\|^2 + \|w\|^2 - 2Re(\langle v|w\rangle) 	\\
	 	  	 		\|v - iw\|^2 = \|v\|^2 + \|w\|^2 + 2Im(\langle v|w\rangle) 		
  	 	 	\end{aligned} 	
  	 	\end{equation*}
  	 	By combinign the two relations in both the real and imaginary case we get that
  	 	\begin{equation*}
  	 		\langle v | w \rangle = \frac{1}{4}\left( \|v + w\|^2 - \|v - w\|^2 + i\|v+iw\|^2 + i\|v-iw\|^2\right).
  	 	\end{equation*}
  	 	This completes the proof.
  	\end{proof} 	
 \medskip \noindent {\textbf{(7.5) } Let $V$ be an inner product space, let $\{e_1, \dots, e_d\} \subset V$ be an orthonormal basis of $V$, and let $\{v_1, \dots, v_d\} \subset V$ be such that 
 $\|v_k - e_k\| < \frac{1}{\sqrt{d}}$ for $1 \leq k \leq d$. Show that $\{v_1, \dots, v_d\}$ is a basis of $V$/

 \begin{proof}
  	We need show that the set is linearly indepeendent to show that it is a basis (given its cardinality). Thus for the sake of contradiction suppose it is not.

  	Then there exist non-trivial $c_k$ so that
  	\begin{equation*}
  	 	0 = c_1 v_1 + \cdots + c_d v_d.
  	 \end{equation*} 
  	We add zero to the sum by subtracting $e_k$ for each $k$ and readding those terms,
  	\begin{equation*}
  	\begin{aligned}
  		0 &= c_1 (v_1 - e_1) + \cdots + c_d (v_d - e_d) + c_1 e_1 + \cdots + c_d e_d. \\
  		-(c_1 e_1 + \cdots + c_d e_d) &= c_1 (v_1 - e_1) + \cdots + c_d (v_d - e_d) + c_1 e_1 + \cdots + c_d e_d. \\
  		\|c_1 e_1 + \cdots + c_d e_d\| &= \|c_1 (v_1 - e_1) + \cdots + c_d (v_d - e_d) + c_1 e_1 + \cdots + c_d e_d\| \\
  		\sqrt{|c_1|^2 \|e_1\|^2 + \cdots + |c_d|^2 \|e_d\|^2} &= \|c_1 (v_1 - e_1) + \cdots + c_d (v_d - e_d) + c_1 e_1 + \cdots + c_d e_d\| \\
  		\sqrt{\sum_{k=1}^d |c_k|^2} &= \|c_1 (v_1 - e_1) + \cdots + c_d (v_d - e_d) + c_1 e_1 + \cdots + c_d e_d\| \\
  		\sqrt{\sum_{k=1}^d |c_k|^2} &\leq \sum_{k=1}^d |c_k| \|v_k - e_k\| < \sum_{k=1}^d \frac{|c_k|}{\sqrt{d}}. \\
  		\sqrt{d}\sqrt{\sum_{k=1}^d |c_k|^2} &< \sum_{k=1}^d |c_k|
  	\end{aligned}
  	\end{equation*}
  	Now consider the following product
  	\begin{equation*}
  		\sum_{k=1}^n |c_k| = |\langle c|1\rangle| \leq \|c\|\|1\| = \sqrt{d} \sqrt{\sum_{k=1}^n |c_k|^2}
  	\end{equation*}
  	which follows from the Cauchy Schwartz equality. This clearly contradicts the foregoing inequality, and therefore it must be the case that the set
  	is linearly independent. Therefore it spans $V$. THis completes the proof.

  \end{proof} 

\medskip \noindent {\textbf{(7.6) } Let $V$ be an inner-product space and let $T: V \to V$. Show that if $T$ is orthogonally diagonalizable, then $T$ is normal.
\begin{proof}
	Recall that $T$ is normal iff $T^*T = TT^*$, thus we wish to show the resultant property. First if $T$ is orthogonally diagonalizable,  then we will first show that $[T]_{\scriptb \to \scriptb}^* =[T^*]_{\scriptb \to \scriptb}.$ Observe that $T(u_j) = t_{1j} b_1 + \cdots a_{nj} b_n$ and since the basis is orthonormal, $a_{kj} = \langle T(b_j)| b_k \rangle.$ Likewise the ${kj}$th element of $T^*$ is given by $\langle T^*(b_j)| b_k \rangle = \overline{\langle b_k| T^*(b_j) \rangle} = \overline{\langle T(b_k)| b_j \rangle} = \overline{a_{jk}}$.

	Now since $T$ is orthogonally diagonalizeable, $[T]_{\scriptb \to \scriptb}$ diagonal implies $[T^*]_{\scriptb \to \scriptb}$ diagonal by the above proof. Thus, $[T^*T]_{\scriptb \to \scriptb} = [T^*]_{\scriptb \to \scriptb} [T]_{\scriptb \to \scriptb} =  [T]_{\scriptb \to \scriptb}  [T^*]_{\scriptb \to \scriptb} =  [TT^*]_{\scriptb \to \scriptb}$ since multiplication by diagonal matrices commutes. In conclusion, $T^*T = TT^*.$
\end{proof}


\medskip \noindent {\textbf{(7.7) }  Let $V$ be a finite-dimensional inner-product space, let $T: V \to V$ be self-adjoint, and let $W \subset V$ be a subspace. Show that $W$ is $T$-invariant iff $W^{\perp}$ is $T$-invariant.

\begin{proof}
	Suppose that $W$ is $T$-invariant. Then $T[W] \subset W$, and in particular for any $w \in W, w' \in W^{\perp}$, $\langle Tw| w'\rangle = 0$. Since $T$ is self-adjoint, we have that
	$\langle Tw| w' \rangle = \langle w| Tw'\rangle = 0.$ Therefore $Tw' \perp w$ and thus $Tw' \in W^\perp$; that is, $T[W^\perp] \subset W^\perp.$ 

	In the other direction if $W^\perp$ is $T$-invariant then for any $w' \in W^{\perp}$ and any $w \in W$ we have that
		$0  = \langle Tw'| w\rangle = \langle w'| Tw \rangle = 0.$
	So, $Tw \in (W^{\perp})^\perp = W$; that is $W$ is $T$-invariant.
\end{proof}

\medskip \noindent {\textbf{(7.8) } Let $V$ be a finite-dimensional inner-product space and let $P: V\to V$ be such that $P^2 = P$ and $P^* = P$. Show that there is a subspace $W \subset V$ such that $P = proj_W.$
\begin{proof}
	Let $W := P[V]$, we want to show that $P(v) \in W$ is the unique element of $W$ such that $v - P(v) \in W^{\perp}$. First, 
	\begin{equation*}
		 \langle P(v) | v - P(v) \rangle =  \langle P^2(v) | v - P(v) \rangle = \langle P(v) | P(v) - P^2(v) \rangle.
	\end{equation*}
	But then as $P^2(v) = P(v)$, we yield that $ \langle P(v) | v - P(v) \rangle =  \langle P(v) | 0 \rangle = 0.$ Therefore $v - P(v) \in W^{\perp},$ and in fact $v - P(v) \in Ker(P).$ 

	Now suppose that there were another element, $y$, in $W$ so that $v - y \in W^\perp$. Then $0 =\langle y | v - y \rangle.$ It follows that $(v -y) + (P(v) - v)  = P(v) - y \in W^{\perp}$, and thus
	 \begin{equation*}
	 	0 = \langle y | P(v) - y \rangle.
	 \end{equation*}
	 Furthermore $P(v) - y \in W$ since $P(v), y \in W$. Therefore\footnote{We use that finite dimensional inner-product spaces are endowed with a natural topology giving them qa Hilbert space structure.} $P(v) - y \in W \cap W^{\perp}$ so $P(v) - y = 0$ and so $P(v) = y$, which is a contradiction. Therefore $P(v)$ is unique, and $P = proj_{W}.$
 \end{proof}

 \medskip \noindent {\textbf{(7.9) } Give an example of an inner-product space $V$ and a subspace $W \subset V$ such that it is not the case that $W \oplus W^\perp = V.$

 \noindent \emph{Soluion.} In the case that the whole space is $C[a,b]$ take $W$ to be all elements $f \in V$ so that $f(a) = 0$. Then if $g$ such that for all $f \in W$, $\langle g, f \rangle = 0$ we must also have that $\langle g, id_{[a,b]} - a \rangle = 0$, but then for arbitrary $q \in V$ we have $\langle h, (id_{[a,b]}  -a) q \rangle = 0$. Thus
 \begin{equation*}
 	0 = \int_a^b (x -a)q(x)g(x)\ dx \implies \int_a^b xq(x)g(x)\ dx = a\int_{a}^b q(x)g(x)\ dx
 \end{equation*}
 In particular take $q  =g $ and then  then $\int_a^b xg^2\ d = 0x$. Since $xg^2$ is a positive, continuous map, we have that $xg^2 = 0$ for all $x$ and thus $g = 0$ on $[a,b]$. Therefore $W^{\perp} = \{0\}$, but it is not the case that $W \oplus \{0\} = V.$
 \end{document}\end
