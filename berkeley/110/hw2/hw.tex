\documentclass{exercises}

\Class{MATH 110 -- Linear Algebra}
\Assignment{Homework 2}
\Date{29 June 2017}

\begin{document}
\InstrBox{
Recall you will receive two grades of equal weight:  one for completeness and one for correctness.  For both parts, each problem is weighted equally.  For the correctness grade, the grader will choose several problems to grade in detail.}

\begin{exr}{}{1}
	\begin{enumerate}
		\item Define the terms \emph{preorder}, \emph{partial-order}, \emph{total-order}, \emph{well-order}, \emph{upper-bound}, and \emph{maximal}.

		\begin{definition}
			Let $P$ be some set and $\leq$ a binary relation on $P$. Then $\leq$ is a preorder if and only if for all $a, b, c \in P$
			\begin{itemize}
				\item It is true that $a \leq a$.
				\item If $a \leq b$ and $b \leq c$ then $a \leq c$.
			\end{itemize}
		\end{definition}


		\begin{definition}
			Let $P$ be some set and $\leq$ a binary relation on $P$. Then $\leq$ is a partial order if and only if for all $a, b, c \in P$
			\begin{itemize}
				\item It is true that $a \leq a$.
				\item If $a \leq b$ and $b \leq c$ then $a \leq c$
				\item If $a \leq b$ and $b \leq a$ then $a = b.$
			\end{itemize}
		\end{definition}


		\begin{definition}
			Let $P$ be some set and $\leq$ a binary relation on $P$. Then $\leq$ is a total order if and only if for all $a, b, c \in P$
			\begin{itemize}
				\item It is true that $a \leq a$.
				\item If $a \leq b$ and $b \leq c$ then $a \leq c$
				\item If $a \leq b$ and $b \leq a$ then $a = b.$
				\item It must be the case that $a \leq b$ or $b \leq a$.
			\end{itemize}
		\end{definition}

		\begin{definition}
			If $(P, \leq)$ is some partially ordered set and $S \subset P$, then $x \in P$ is an upper bound of $S$ if and only if for all $s \in S$, $s \leq x.$
		\end{definition}

		\begin{definition}
			If $(P, \leq)$ is some partially ordered set and $S \subset P$, then $x \in P$ is a maximal element of $S$ if and only if for all $s \in S$, $x \leq s$ imples $m = s.$
		\end{definition}
		\item State Zorn's Lemma.
		\begin{lemma}
			If $(P, \leq)$ is a partially ordered set such that every totally ordered subset $T \subset P$ has an upper bound, then $P$ contains at least one maximal element.
		\end{lemma}
		\item Use Zorn's Lemma to prove that every vector space has a basis.
		\emph{Proof. }
			If $V = \{0\}$ then $0$ is trivially a basis for $V$. Otherwise suppose that $V$ contains an element $v$, then $\{v\}$ is a linearly independent subset of $V$. Take $A \subset V$ to be a linearly indepdendent subset and let $(I, \subset)$ be the set of linearly independent subsets containing $A$. The relation $\subset$ endows $I$ with a partial order. 

			To apply Zorns lemma we need show that for every totally ordered subset $T \subset I$, $T$ has a maximal element; that is to say, for every totally ordered subcollection $T$ of $I$, then there is a linearly independent set $\kappa \in I$ so that for all $\tau \in T$, $\tau \subset \kappa$. Let $\kappa = \bigcup_{\tau \in T} \tau.$ Then we want to show that $\kappa$ is  linearly independent. 

			Suppose for the sake of contradiction that $\kappa$ is linearly dependent, then their is some $v \in \kappa$ so that
			\begin{equation}\label{eq:linindepzorn}
			 	v = \sum_{k=1}^n \alpha_k \cdot v_k.
			 \end{equation} 
			Then since $\kappa$ is the union of all $\tau$, $v \in \tau_0 \in T$, and $v_k \in \tau_k \in T$. Since $T_n = \{\tau_0, \cdots, \tau_n\}$ is finite and a subset of a totally ordered set, it is a finite totally ordered set. It follows that $T_n$ attains a maximum under $\subset$, say $\tau_j$. Then $v, v_k \in \tau_j$ for all $k \leq n$. By the linear independence of $\tau_j \in I$ it follows that there are no $\alpha_k$ so that \eqref{eq:linindepzorn} is true, but this contradicts the conclusion that is. Therefore by contradiction $\kappa$ must be linearly independent. Since for all $\tau \in T$, $\tau \subset \kappa$ then $\kappa$ is an upper bound for $T$. By Zorn's lemma $I$ has a maximal element $\scriptb$. 

			Now we need only show that $\scriptb$ is a span. Suppose for the sake of contradiction that $v \in V$ and $v \not \in span(\scriptb)$. Then there are no $\alpha_k$ so that $v = \sum_{k=1}^n \alpha_k v_k$ with $v_k \in \scriptb$. Thus $\scriptb \cup \{v\} \supset \scriptb$ is linearly independent but this contradict the maximality of $\scriptb$ and therefore $span(\scriptb) = V$. Thus $\scriptb$ is a basis.

		\item Given an example of a $\K$-module which has no basis.
	\end{enumerate}
	\begin{rmk}
		Hint:  You will need to look in the notes for this as not everything is covered in lecture.  In an ideal world, I would be able to talk about everything, but we don't live in an ideal world, and if you are a serious mathematics student, you will need to get in the habit of learning on your own whatever you need to solve any problem you might encounter.  Only knowing what is covered in lecture is not going to cut it if you want to be a mathematician.
	\end{rmk}
\end{exr}



\begin{exr}{}{2}
	Let $V$ be a vector space and let $S\subseteq V$.
	\begin{enumerate}
		\item Show that if $\abs{S}>\dim (V)$, then $S$ is linearly-dependent.\\

		\emph{Proof.} Suppose that $S$ is linearly independent. Recall that in our previous proof we showed that  any linearly indepdendent set $S$ can be extended to a basis $\scriptb_S$. Then $\dim(V) \ni |\scriptb_S| > |S|$, but then this contradicts $\abs{S} > \dim(V)$ if $\dim(V)$ is only a single cardinality. The fact that $\dim(V)$ is a single cardinality follows from the fundamental theorem of dimension in Gleezy's notes. Therefore $S$ must be linearly-dependent.

		\item Suppose that $|S| < \dim (V)$. Then show that $S$ does not span $V$.\\

		\emph{Proof.} Recall in our proof in 1.(iv) we showed taht any linearly independent set $S$ there is an extension to a basis $\scriptb_S$ which contains $S$. By $|S| < \dim (V)$ then there is not a bijection $S \sim \scriptb_S$ and in particular there is a $v \in \scriptb$ with $v \not \in \scriptb_S$. By linear independence of $\scriptb_S$ there are no $\alpha$ so that $v = \sum_{k=1}^n{\alpha_v v_k}$. In particular $v$ is not a linear combination of any finite subset of $S$, and thus $v \not \in span(S)$. This completes the proof. 
		
	\end{enumerate}
\end{exr}

\begin{exr}{}{3}
	Let $V$ be a vector space and let $S\subseteq V$.
	\begin{enumerate}
		\item Suppose that $V$ is finite-dimensional, that $\abs{S}=\dim (V)$, and that $S$ spans $V$.  Show that $S$ is a basis. \\
		\emph{Proof.} Suppose that $S$ is linearly-dependent. Then there is $v \in S$ so that $span(S \setminus \{v\}) = V$. Repeat this process until $S$ is linearly-independent. This process has an end since $S$ is finite. Then since $\dim (V)$ is finite, removal of elements from $S$, breaks bijectivity with $\dim (V)$. Therefore $dim(V) > |S|,$ but this contradicts $|S| = \dim(V).$ Therefore $S$ must be linearly independent. Therefore $S$ is a basis.

		\item Suppose that $V$ is finite-dimensional, that $\abs{S}=\dim (V)$, and that $S$ is linearly-independent.  Show that $S$ is a basis. \\


		\emph{Proof.} Recall in our proof in 1.(iv) we showed that any linearly independent set $S$ there is an extension to a basis $\scriptb_S$ which contains $S$. Since $V$ is finite dimensional $|\scriptb_S| = \dim(V)$ is finite.  Since $|S| = \dim(V)$ and $\scriptb_S \supset S$ it follows that $\scriptb_S \sim dim(V) \sim \scripts$ implies that $S = \scriptb_S$, and so $S$ is a basis.
		\item Find counter-examples to show that both of these fail if $V$ is infinite-dimensional.
	\end{enumerate}
\end{exr}

\begin{exr}{}{4}
	Let $V$ be a vector space and let $W\subseteq V$ be a subspace.
	\begin{enumerate}
		\item Show that if $V$ is finite-dimensional and $\dim (W)=\dim (V)$, then $V=W$. \\

		\emph{Proof.} If $\dim(W) = \dim(V)$ then take $\scriptb_W$ to be some basis of $W$. Then $|\scriptb_W| = \dim(V)$ and $\scriptb_W$ is linearly-independent. Then by the previous exercise $\scriptb$ msut be a basis for $V$ and thus $W = span(\scriptb_W) = V$. This completes the proof.


		\item Find a counter-example to show that this is false if $V$ is infinite-dimensional. \\

		\emph{Counter-Example.} Endow $V = \mathbb{R}^\infty \oplus \mathbb{R}^\infty$ with some basis $\scriptb$. Then let $W \subset V$ so that $W = \mathbb{R}^\infty \oplus \{0\}.$ The basis $W$, say $\scriptc$, is infinite and $\scriptb$ is infinite, both with countable cardinality, and yet $W \neq V$.
	\end{enumerate}
\end{exr}

\begin{exr}{}{5}
	Let $T\colon V\rightarrow W$ be a linear-transformation of vector spaces with $\dim (V)=\dim (W)$.
	\begin{enumerate}
		\item Suppose that $V$ and $W$ are finite-dimensional.  Show that $T$ is injective iff it is surjective. \\]

		\emph{Proof.} If $V, W$ are finite dimensioinal, then $T$ is injective iff $\dim(Ker(T)) = 0$ which by the rank nullility thoerem is if and only if $dim(V) = 0 + dim(Im(T)) = dim(W)$ by the the hypothesis which implies that $T$ is surjective. On the other hand $T$ is injective if and only if $dim(Im(T)) = dim(W) = dim(V) = dim(Im(T)) + dim(Ker(T))$ and thus $dim(Ker(T)) = 0$ implies that $T$ is injective.

		\item Find a counter-example to show that this fails in infinite-dimensions.
	\end{enumerate}
\end{exr}

\begin{exr}{}{6}
	Let $V$ and $W$ be finite-dimensional vector spaces and let $T\colon V\rightarrow W$ be a linear-transformation.
	\begin{enumerate}
		\item Show that if $\dim (V)>\dim (W)$, then $T$ is not injective. \\

		\emph{Proof.}
		If $dim(V) > dim(W)$ then $dim(V) = dim(Ker(T)) + dim(Im(T))$. Now supose that $T$ is injective then $dim(V) = dim((Im(T)) \leq dim(W)$ but this a contradiction and therefore $T$ must not be injective.
		\item Show that if $\dim (W)>\dim (V)$, then $T$ is not surjective. \\

		\emph{Proof.}
		If $dim(W) > dim(V)$ then $dim(V) = dim(Ker(T)) + dim(Im(T)).$ Now suppose that $T$ is surjective then $dim(V) = dim(W) + dim(Ker(T))$ and thus $dim(V) \geq dim(W)$ which is a contraqiction. Therefore $T$ is not sirjective.
	\end{enumerate}
\end{exr}

\begin{exr}{}{7}
	Define
	\begin{equation*}
		\basis{B}\ceqq \left\{ \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix},\begin{bmatrix}1 \\ 0 \\ -1\end{bmatrix},\begin{bmatrix}1 \\ 2 \\ 2\end{bmatrix}\right\}
	\end{equation*}
	and
	\begin{equation*}
		\basis{C}\ceqq \left\{ \begin{bmatrix}2 \\ -3 \\ -7\end{bmatrix},\begin{bmatrix}6 \\ 0 \\ 0\end{bmatrix},\begin{bmatrix}-2 \\ -2 \\ 3\end{bmatrix}\right\} .
	\end{equation*}
	\begin{enumerate}
		\item Check that $\basis{B}$ and $\basis{C}$ are bases of $\R ^3$. \\


		We show that the transformations $[\scriptb]_{\scriptb \to \scripte}, [\scriptc]_{\scriptb \to \scripte}$ have trivial kernels, and thus that they are injective, then the results from the previous exercise give surjectivity. Under such conditions $[\scriptb]_\cdot, [\scriptc]_\cdot$ being isomorphisms give that $\scriptb$ and $\scriptc$ are basis for $\mathbb{R}^3$ since the only linear combination yielding $0$ is the trivial one (follows from trivial kernel), and all linear combinations span the space (follows froms surjectivity). \\

		Let us consider the kernel of $[\scriptb]_\cdot$ as found by row reduction.
Starting matrix
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&0&\\1&0&2&0&\\1&-1&2&0&\\\end{array}
\right\}
\end{equation}Add -1 times row 1 to row 2
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&0&\\0&-1&1&0&\\1&-1&2&0&\\\end{array}
\right\}
\end{equation}Add -1 times row 1 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&0&\\0&-1&1&0&\\0&-2&1&0&\\\end{array}
\right\}
\end{equation}Multiply row 2 by -1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&0&\\0&1&-1&0&\\0&-2&1&0&\\\end{array}
\right\}
\end{equation}Add -1 times row 2 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&2&0&\\0&1&-1&0&\\0&-2&1&0&\\\end{array}
\right\}
\end{equation}Add 2 times row 2 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&2&0&\\0&1&-1&0&\\0&0&-1&0&\\\end{array}
\right\}
\end{equation}Multiply row 3 by -1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&2&0&\\0&1&-1&0&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}Add -2 times row 3 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0&\\0&1&-1&0&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}Add 1 times row 3 to row 2
and the final solution is
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0&\\0&1&0&0&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}
Therefore there is a unique solution ($0$) and thus the Kernel is trivial. Therefore $\scriptb$ is a basis. \\

Next we perform a similar computation on $\scriptc$.
Starting matrix
\begin{equation}
\left\{
    \begin{array}{ccc|cl}2&6&-2&0&\\-3&0&-2&0&\\-7&0&3&0&\\\end{array}
\right\}
\end{equation}Multiply row 1 by 0.5
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0&\\-3&0&-2&0&\\-7&0&3&0&\\\end{array}
\right\}
\end{equation}Add 3 times row 1 to row 2
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0&\\0&9&-5&0&\\-7&0&3&0&\\\end{array}
\right\}
\end{equation}Add 7 times row 1 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0&\\0&9&-5&0&\\0&21&-4&0&\\\end{array}
\right\}
\end{equation}Multiply row 2 by 0.111111111111111
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0&\\0&1&-0.555555555555556&0&\\0&21&-4&0&\\\end{array}
\right\}
\end{equation}Add -3 times row 2 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0&\\0&21&-4&0&\\\end{array}
\right\}
\end{equation}Add -21 times row 2 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0&\\0&0&7.66666666666667&0&\\\end{array}
\right\}
\end{equation}Multiply row 3 by 0.130434782608696
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}Add -0.666666666666667 times row 3 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0&\\0&1&-0.555555555555556&0&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}Add 0.555555555555556 times row 3 to row 2
and the final solution is
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0&\\0&1&0&0&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}
Therefore there is a unique solution $(0)$ and the kernel is trivial.

		\item Compute the change of basis matrix $[\id]_{\basis{C}\leftarrow \basis{B}}$. \\

		We compute the change of basis matrix by computing row reduction each time on $[\scriptc]x = \scriptb_i.$ What follows is the computation.

		Starting matrix
\begin{equation}
\left\{
    \begin{array}{ccc|cl}2&6&-2&1&\\-3&0&-2&1&\\-7&0&3&1&\\\end{array}
\right\}
\end{equation}Multiply row 1 by 0.5
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\-3&0&-2&1&\\-7&0&3&1&\\\end{array}
\right\}
\end{equation}Add 3 times row 1 to row 2
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&9&-5&2.5&\\-7&0&3&1&\\\end{array}
\right\}
\end{equation}Add 7 times row 1 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&9&-5&2.5&\\0&21&-4&4.5&\\\end{array}
\right\}
\end{equation}Multiply row 2 by 0.111111111111111
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&1&-0.555555555555556&0.277777777777778&\\0&21&-4&4.5&\\\end{array}
\right\}
\end{equation}Add -3 times row 2 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&-0.333333333333333&\\0&1&-0.555555555555556&0.277777777777778&\\0&21&-4&4.5&\\\end{array}
\right\}
\end{equation}Add -21 times row 2 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&-0.333333333333333&\\0&1&-0.555555555555556&0.277777777777778&\\0&0&7.66666666666667&-1.33333333333333&\\\end{array}
\right\}
\end{equation}Multiply row 3 by 0.130434782608696
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&-0.333333333333333&\\0&1&-0.555555555555556&0.277777777777778&\\0&0&1&-0.173913043478261&\\\end{array}
\right\}
\end{equation}Add -0.666666666666667 times row 3 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&-0.217391304347826&\\0&1&-0.555555555555556&0.277777777777778&\\0&0&1&-0.173913043478261&\\\end{array}
\right\}
\end{equation}Add 0.555555555555556 times row 3 to row 2
and the final solution is
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&-0.217391304347826&\\0&1&0&0.181159420289855&\\0&0&1&-0.173913043478261&\\\end{array}
\right\}
\end{equation}

Next
\begin{equation}
\left\{
    \begin{array}{ccc|cl}2&6&-2&1&\\-3&0&-2&0&\\-7&0&3&-1&\\\end{array}
\right\}
\end{equation}Multiply row 1 by 0.5
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\-3&0&-2&0&\\-7&0&3&-1&\\\end{array}
\right\}
\end{equation}Add 3 times row 1 to row 2
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&9&-5&1.5&\\-7&0&3&-1&\\\end{array}
\right\}
\end{equation}Add 7 times row 1 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&9&-5&1.5&\\0&21&-4&2.5&\\\end{array}
\right\}
\end{equation}Multiply row 2 by 0.111111111111111
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&1&-0.555555555555556&0.166666666666667&\\0&21&-4&2.5&\\\end{array}
\right\}
\end{equation}Add -3 times row 2 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0.166666666666667&\\0&21&-4&2.5&\\\end{array}
\right\}
\end{equation}Add -21 times row 2 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0.166666666666667&\\0&0&7.66666666666667&-1&\\\end{array}
\right\}
\end{equation}Multiply row 3 by 0.130434782608696
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0.166666666666667&\\0&0&1&-0.130434782608696&\\\end{array}
\right\}
\end{equation}Add -0.666666666666667 times row 3 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0.0869565217391304&\\0&1&-0.555555555555556&0.166666666666667&\\0&0&1&-0.130434782608696&\\\end{array}
\right\}
\end{equation}Add 0.555555555555556 times row 3 to row 2
and the final solution is
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0.0869565217391304&\\0&1&0&0.0942028985507246&\\0&0&1&-0.130434782608696&\\\end{array}
\right\}
\end{equation}


Next

And thus the solution is 
\begin{equation}
\begin{bmatrix}
	-0.217391304347826 &  0.0869565217391304  &  -0.434782608695652 \\
0.181159420289855 &  0.0942028985507246  & 0.195652173913043 \\
-0.173913043478261 &  -0.130434782608696  & -0.347826086956522
\end{bmatrix}
\end{equation}



		\item Compute $[v]_{\basis{B}}$, where $v\ceqq \coord{1,0,0}\in \R ^3$.

		Starting matrix
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&1&\\1&0&2&0&\\1&-1&2&0&\\\end{array}
\right\}
\end{equation}Add -1 times row 1 to row 2
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&1&\\0&-1&1&-1&\\1&-1&2&0&\\\end{array}
\right\}
\end{equation}Add -1 times row 1 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&1&\\0&-1&1&-1&\\0&-2&1&-1&\\\end{array}
\right\}
\end{equation}Multiply row 2 by -1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&1&1&1&\\0&1&-1&1&\\0&-2&1&-1&\\\end{array}
\right\}
\end{equation}Add -1 times row 2 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&2&0&\\0&1&-1&1&\\0&-2&1&-1&\\\end{array}
\right\}
\end{equation}Add 2 times row 2 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&2&0&\\0&1&-1&1&\\0&0&-1&1&\\\end{array}
\right\}
\end{equation}Multiply row 3 by -1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&2&0&\\0&1&-1&1&\\0&0&1&-1&\\\end{array}
\right\}
\end{equation}Add -2 times row 3 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&2&\\0&1&-1&1&\\0&0&1&-1&\\\end{array}
\right\}
\end{equation}Add 1 times row 3 to row 2
and the final solution is
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&2&\\0&1&0&0&\\0&0&1&-1&\\\end{array}
\right\}
\end{equation}.

The solution is therefore 
\begin{equation*}
 [v]_{\basis{\scriptb}} =
 \begin{bmatrix}
 	2 \\ 0 \\ -1
 \end{bmatrix}
\end{equation*}



		\item compute $[v]_{\basis{C}}$.

		Starting matrix
\begin{equation}
\left\{
    \begin{array}{ccc|cl}2&6&-2&1&\\-3&0&-2&0&\\-7&0&3&0&\\\end{array}
\right\}
\end{equation}Multiply row 1 by 0.5
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\-3&0&-2&0&\\-7&0&3&0&\\\end{array}
\right\}
\end{equation}Add 3 times row 1 to row 2
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&9&-5&1.5&\\-7&0&3&0&\\\end{array}
\right\}
\end{equation}Add 7 times row 1 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&9&-5&1.5&\\0&21&-4&3.5&\\\end{array}
\right\}
\end{equation}Multiply row 2 by 0.111111111111111
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&3&-1&0.5&\\0&1&-0.555555555555556&0.166666666666667&\\0&21&-4&3.5&\\\end{array}
\right\}
\end{equation}Add -3 times row 2 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0.166666666666667&\\0&21&-4&3.5&\\\end{array}
\right\}
\end{equation}Add -21 times row 2 to row 3
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0.166666666666667&\\0&0&7.66666666666667&0&\\\end{array}
\right\}
\end{equation}Multiply row 3 by 0.130434782608696
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0.666666666666667&0&\\0&1&-0.555555555555556&0.166666666666667&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}Add -0.666666666666667 times row 3 to row 1
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0&\\0&1&-0.555555555555556&0.166666666666667&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}Add 0.555555555555556 times row 3 to row 2
and the final solution is
\begin{equation}
\left\{
    \begin{array}{ccc|cl}1&0&0&0&\\0&1&0&0.166666666666667&\\0&0&1&0&\\\end{array}
\right\}
\end{equation}

Therefore the solution is $[0, 0.1666666, 0].$ \\[2in]
	\end{enumerate}
\end{exr}

\begin{exr}{}{8}
	Define
	\begin{equation*}
		V\ceqq \left\{ f\in C^{\infty}(\R ):f''+f=0\right\} ,
	\end{equation*}
	so that $V$ is a complex vector space with bases
	\begin{equation*}
		\basis{B}\ceqq \{ \e ^{\im x},\e ^{-\im x}\} \text{ and }\{ \cos (x),\sin (x)\} .
	\end{equation*}
	Define $D\colon V\rightarrow V$ by $D(f)\ceqq f'$.
	\begin{enumerate}
		\item Check that indeed $D(f)\in V$ if $f\in V$. \\
		\emph{Proof.} Take any $f \in V$. Then $f'' + f = 0$ and $(f'' + f)' = 0' = 0 = f''' + f'$ and thus $f' \in V$ and thus $D: V\ to V$.
		\item Compute $[D]_{\basis{B}\leftarrow \basis{B}}$. \\
		\emph{Proof}. Skipping all of the boring bullshit, the answer is
		\begin{equation*}
			\begin{bmatrix}
				i & 0 \\
				0 & -i
			\end{bmatrix}
		\end{equation*}

		\item Compute $[D]_{\basis{C}\leftarrow \basis{B}}$. \\
		\emph{Proof}. Skipping all of the intermediate prescriptions, the answer\footnote{I didn't google this shit.} is 

		\begin{equation*}
			\begin{bmatrix}
				i & -i \\
				-1 & -1 \\
			\end{bmatrix}
		\end{equation*}


	\end{enumerate}
	\begin{rmk}
		Of course, there are two other combinations I could have asked you to compute ($\basis{B}\leftarrow \basis{C}$ and $\basis{C}\leftarrow \basis{C}$), but I decided to be a nice guy and not have you do essentially the same work four times over.  You're welcome ;-)
	\end{rmk}
\end{exr}

\begin{exr}{}{9}
	Let $\F$ be a field, let $V$ be a vector space over $\F$, let $\basis{B}$ be a basis for $V$, for every $b\in \basis{B}$, let $b^*\colon V\rightarrow \F$ be the unique linear-transformation such that $b^*(b)=1$ and $b^*(c)=0$ for all $c\in \basis{B}$, $c\neq b$, and define $\basis{B}^*\ceqq \{ b^*\in V^*:b\in \basis{B}\}$.\footnote{Recall that $V^*\ceqq \Mor _{\Vect _{\F}}(V,\F )$ is the dual space of $V$.}
	\begin{enumerate}
		\item Show that $\basis{B}^*\subseteq V^*$ is linearly-independent. \\

		\emph{Proof.} Suppose not then there is  $v^* \in \scriptb^*$ with $v^* = \sum_{k=1}^n \alpha_k b_k^*,$ with $b_k^* \neq v^*$ and then $v^*(v) = 1 = \sum_{k=1}^n \alpha b_k^*(v) = \sum_{k=1}^n \alpha 0  = 0$ which is a contradiction, and thuis $\scriptb^*$ must be linearly-independenrt.\\
		
		\item Show that $\basis{B}^*$ is a basis of $V^*$ if $V$ is finite-dimensional (so that in this case $\dim (V)=\dim (V^*)$).
		\item Find a counter-example to show that $\basis{B}^*$ will not span $V^*$ in general (if $V$ is infinite-dimensional).
	\end{enumerate}
	\begin{rmk}
		If $V$ is finite-dimensional, $\basis{B}^*$ is called the \term{basis of $V^*$ dual to $\basis{B}$}.
	\end{rmk}
\end{exr}

\vspace{-10pt} % To suppress random emptypage at the end

\end{document}