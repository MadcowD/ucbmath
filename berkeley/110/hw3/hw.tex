\documentclass[11pt]{amsart}

\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{fullpage}
\usepackage{tikz-cd}
% \usepackage{euscript}
% \makeatletter
% \nopagenumbers
\usepackage{verbatim}
\usepackage{color}
\usepackage{hyperref}

\usepackage{fullpage,tikz,float}
%\usepackage{times} %, mathtime}

\textheight=600pt %574pt
\textwidth=480pt %432pt
\oddsidemargin=15pt %18.88pt
\evensidemargin=18.88pt
\topmargin=10pt %14.21pt

\parskip=1pt %2pt

% define theorem environments
\newtheorem{theorem}{Theorem}    %[section]
%\def\thetheorem{\unskip}
\newtheorem{proposition}[theorem]{Proposition}
%\def\theproposition{\unskip}
\newtheorem{conjecture}[theorem]{Conjecture}
\def\theconjecture{\unskip}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{sublemma}[theorem]{Sublemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{observation}[theorem]{Observation}
%\def\thelemma{\unskip}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\def\thedefinition{\unskip}
\newtheorem{notation}[definition]{Notation}
\newtheorem{remark}[definition]{Remark}
% \def\theremark{\unskip}
\newtheorem{question}[definition]{Question}
\newtheorem{questions}[definition]{Questions}
%\def\thequestion{\unskip}
\newtheorem{example}[definition]{Example}
%\def\theexample{\unskip}
\newtheorem{problem}[definition]{Problem}
\newtheorem{exercise}[definition]{Exercise}

\numberwithin{theorem}{section}
\numberwithin{definition}{section}
\numberwithin{equation}{section}

\def\reals{{\mathbb R}}
\def\torus{{\mathbb T}}
\def\integers{{\mathbb Z}}
\def\rationals{{\mathbb Q}}
\def\naturals{{\mathbb N}}
\def\complex{{\mathbb C}\/}
\def\distance{\operatorname{distance}\,}
\def\support{\operatorname{support}\,}
\def\dist{\operatorname{dist}\,}
\def\Span{\operatorname{span}\,}
\def\degree{\operatorname{degree}\,}
\def\kernel{\operatorname{kernel}\,}
\def\dim{\operatorname{dim}\,}
\def\codim{\operatorname{codim}}
\def\trace{\operatorname{trace\,}}
\def\dimension{\operatorname{dimension}\,}
\def\codimension{\operatorname{codimension}\,}
\def\nullspace{\scriptk}
\def\kernel{\operatorname{Ker}}
\def\p{\partial}
\def\Re{\operatorname{Re\,} }
\def\Im{\operatorname{Im\,} }
\def\ov{\overline}
\def\eps{\varepsilon}
\def\lt{L^2}
\def\curl{\operatorname{curl}}
\def\divergence{\operatorname{div}}
\newcommand{\norm}[1]{ \|  #1 \|}
\def\expect{\mathbb E}
\def\bull{$\bullet$\ }
\def\det{\operatorname{det}}
\def\Det{\operatorname{Det}}
\def\rank{\mathbf r}
\def\diameter{\operatorname{diameter}}

\def\t2{\tfrac12}

\newcommand{\abr}[1]{ \langle  #1 \rangle}

\def\newbull{\medskip\noindent $\bullet$\ }
\def\field{{\mathbb F}}
\def\cc{C_c}



% \renewcommand\forall{\ \forall\,}

% \newcommand{\Norm}[1]{ \left\|  #1 \right\| }
\newcommand{\Norm}[1]{ \Big\|  #1 \Big\| }
\newcommand{\set}[1]{ \left\{ #1 \right\} }
%\newcommand{\ifof}{\Leftrightarrow}
\def\one{{\mathbf 1}}
\newcommand{\modulo}[2]{[#1]_{#2}}

\def\bd{\operatorname{bd}\,}
\def\cl{\text{cl}}
\def\nobull{\noindent$\bullet$\ }

\def\scriptf{{\mathcal F}}
\def\scriptq{{\mathcal Q}}
\def\scriptg{{\mathcal G}}
\def\scriptm{{\mathcal M}}
\def\scriptb{{\mathcal B}}
\def\scriptc{{\mathcal C}}
\def\scriptt{{\mathcal T}}
\def\scripti{{\mathcal I}}
\def\scripte{{\mathcal E}}
\def\scriptv{{\mathcal V}}
\def\scriptw{{\mathcal W}}
\def\scriptu{{\mathcal U}}
\def\scriptS{{\mathcal S}}
\def\scripta{{\mathcal A}}
\def\scriptr{{\mathcal R}}
\def\scripto{{\mathcal O}}
\def\scripth{{\mathcal H}}
\def\scriptd{{\mathcal D}}
\def\scriptl{{\mathcal L}}
\def\scriptn{{\mathcal N}}
\def\scriptp{{\mathcal P}}
\def\scriptk{{\mathcal K}}
\def\scriptP{{\mathcal P}}
\def\scriptj{{\mathcal J}}
\def\scriptz{{\mathcal Z}}
\def\scripts{{\mathcal S}}
\def\scriptx{{\mathcal X}}
\def\scripty{{\mathcal Y}}
\def\frakv{{\mathfrak V}}
\def\frakG{{\mathfrak G}}
\def\aff{\operatorname{Aff}}
\def\frakB{{\mathfrak B}}
\def\frakC{{\mathfrak C}}

\def\symdif{\,\Delta\,}
\def\mustar{\mu^*}
\def\muplus{\mu^+}

\def\soln{\noindent {\bf Solution.}\ }


%\pagestyle{empty}
%\setlength{\parindent}{0pt}

\begin{document}

\begin{center}{\bf Math 110 --- Homework 3 --- UCB, Summer 2017 --- William Guss}
\end{center}

\medskip \noindent {\textbf{(3.1) } 
\begin{itemize}
	\item Show that $V = W_1 \oplus \cdots \oplus W_m$ if and only if $V = W_1 + \cdots + W_m$ and $0 = w_1 + \cdots + w_m$, $w_k \in W_k$ implies that $w_1, \cdots,w_m = 0$.

	\begin{proof}
		Suppose that $V$ is the given direct sum. Then if $0 = w_1 + \cdots + w_m$ each $w_k$ is unique. Then $0 \in W_k$ for all $k$ and $0 + \cdots + 0 = 0 \in V$ gives that this is the only such linear combination yielding $0$, by uniqueness. Therefore $w_k = 0$ for all $w_k$.

		Now suppose that there is a vector $v \in V$ with non unique decomposition; that is, let $w_k, z_k \in W_k$ for all $k$, then $v$ is such that
		\begin{equation*}
			v = w_1 + \cdots + w_m = z_1 +\cdots + z_m
		\end{equation*}
		where there exists a $j$ so that $W_j \ni w_j \neq z_j \in W_j$.

		Then it follows that \begin{equation*}
			\begin{aligned}
				0 = 0+\cdots +0 &= v +(-v)\\  &= w_1 + \cdots + w_m + -(w_1 + \cdots + w_m) \\
				&= w_1 -z_1 + \cdots + w_m - z_m.
			\end{aligned}
		\end{equation*}
		Since $\mathbb{K}$ is a ring, and thus addition is commutative. Since $w_j \neq v_j,$ then $w_j - v_j \neq 0$ and thus there are non-zero terms in the decomposition of $0$. Therefore,
		$0 = d_1 + \cdot + d_m$ does not imply $d_k = 0$ for all $k$. Thus by contraposition we have shown the other direction. This completes the proof.
	\end{proof}

	\item Show that $V = U \oplus W$ iff ($U + W = V$ and $U \cap W = 0$). 
	\begin{proof}
		Suppose that $V = U \oplus W$. If $v \in U \cap W$, by definition $v = u + w$ uniquely. Futhermore there are $u' \in U$ and $w' \in W$ so that $u' = u + w = w'$.  Thus $0 = v + (-v) = (u' - u) - w = -u + (w' - w)$ with $(w' -w) \in W$ and $(u' -u) \in U.$ Using the uniqueness of $0$ in its direct sum decomposition $(u' -u) = u'$ and $(w' -w) = w'$. Therefore $-w = 0$ and $-u = 0$, so $v = 0 + 0$ implies $v = 0,$ and $U \cap V = 0$. Next $V = U\oplus W$ implies that every $v \in V$ has a unique decomposition in $U \oplus V$ and so $V \subset U + W$. On the other hand $U + W \subset V$ as $W \subset V$ and $U \subset V$ as subspaces, and therefore any combination of vectors in both need be in $V$ as to not violate $V$ being a $\mathbb{K}$-module.

		In the other direction suppose that $U+W = V$ and $U \cap W = 0$. Now take $v \in V$ so that $v =u + w$. We will show that $u,w \in U, W$ are unique.  Suppose there were $u', w' \in U, W$ with $v = u'  + w'$ and $(u,w) \neq (u',w')$ for the sake of contradiction. Then $0 = v + (-v) \in U \cap V$ with $0= (u-u') + (w-w')$, but one of thes terms must be non-zero by our supposition, and therefore the other must be its inverse. That is, $(u -u') = -(w-w')$ and so $(u -u') \in W$ and $(w - w') \in U$ by $U,W$ subspaces. So $w - w' =0$ and $u - u' = 0$ by the hypothesis, which contracits $(u,w) \neq (u',w')$. Therefore the decompositon $v = u + w$ is unique and $V = U + W$ is direct.
	\end{proof}
\end{itemize}

\medskip \noindent {\textbf{(3.2) } 
\begin{itemize}
\item Show that $Ker(T)$ is $T$-invariant.
\begin{proof}
	We use the alternative definition for $T$-invariant given in $4.4.85$. Then if $w \in Ker(T)$, $T(w) = 0$, and since $T(T(w)) = T(0)= 0$ we have $T(w) \in Ker(T)$. Therefore $Ker(T)$ is $T$-invariant.
\end{proof}
\item Show that $Im(T)$ is $T$-invariant.
\begin{proof}
	Recall that as a subspace $Im(T) \subset V$. Then $T(Im(V)) \subset \{v \in V \ :\ v = T(w), w \in V\}$; since $w \in Im(V)$ gives us $T(Im(V)).$  Therefore $T(Im(V)) \subset Im(v)$ and $Im(V)$ is $T$-invariant.
\end{proof}
\end{itemize}


\medskip \noindent {\textbf{(3.3) } Let $V$ be a vector space over $\mathbb{F}$, let $X, H: V \to V$ be linear, let $\alpha, \lambda \in \mathbb{F}$ be central, let $v \in V$ be an eigenvector of $H$ with eigenvalue $\lambda.$ Show that if $H \circ X - X \circ H = \alpha X$ then $X(v)$ is an eigen vector of $H$ with eigenvalue $\alpha + \lambda.$
\begin{proof}
  	Using theorem $4.2.12$ we yield that $\lambda$ need be central and $H(v) = \lambda v$. Then $\alpha X (v) = H \circ X(v) - X \circ H(v) = H(X(v)) - X(\lambda v).$ Then using centrality of $\lambda$ we get $H(X(v)) = (\alpha + \lambda) X(v)$; and thus $\alpha + \lambda$ is an eigenvalue of $H$, with $X(v) \in Eig_{\lambda, H}.$
  \end{proof}  



\medskip \noindent {\textbf{(3.4) } 


\medskip \noindent {\textbf{(3.5) } 
\begin{itemize}
	\item Find the eigenvalues and eigenspaces of both $L$ and $R$. \footnote{Gleezy actually got the operators wrong in his notes, but I'll just keep consistent with them; that is, $L(0,1,2,\cdots) = (0,0,1,2,3,4, \cdots)$.}
	\\

	\noindent \emph{Solution.} For the right shift operator, every $r \in \mathbb{R}$ is an eigenvalue. To see this take $c \in \mathbb{C}^\mathbb{N}$ so that $c = \langle z, rz, r^2z, r^3z, \cdots\rangle.$ with $z \in \mathbb{C}.$ Then $R(c) = \langle  rz, r^2z, r^3z, \cdots\rangle = rc.$ The cooresponding eigen spaces are $ Eig_{r, R} = \{\langle z, rz, r^2 z,\cdots\rangle \ :\ z \in \mathbb{C} \}. $

	For the left shift operator, we claim that there are no eigenvalues. To see this suppose that there were some non-zero eigenvalue, say $r = \lambda.$ Then if $c \in Eig_{\lambda, L}$ non-trivially we have $L(c) = rc$. Take the first non-zero element in $c$, say $c_j$, and then $L(c)_j = 0$ and so it could not be that $r c_j = 0$ unless $r = 0$. Now suppose that $r = 0$ were an eigenvalue and take a non-zero eigenvector $c \in Eig_{0, L}.$ Then there is a $j$ so that $c_j \neq 0$ and then $L(c)_j = 0$, but $L(c)_{j+1} \neq 0$ which contradicts that $L(c) = rc = 0.$ Therefore $r = 0$ is not an eigenvalue of $L$.\\


	\item Find the eigenvalues and eigenspaces of both $L|_{\ell^2(\mathbb{C}))}, R|_{\ell^2(\mathbb{C})}: \ell^2(\mathbb{C}) \to \ell^2(\mathbb{C}).$\\

	\noindent \emph{Solution.} First let $T: V \to V$ as vector spaces over a division ring $\mathbb{K}.$ Then $T$ has no eigenvalues if and only if there are no $W \subset V$ with $T|_W = r id_W$ for some $r \in \mathbb{K}$. Therefore if $E$ is a subspace $V$ then $T|_E$ has no eigenvalues as any subspace of $E$, say $J$, is mereley a subspace of $V$ expressed $J = E \cap J$. Therefore $L|_{\ell^2(\mathbb{C})}$ has no eigenvalues using this spectral subspace principle. 

	We apply similar reasoning as above to $R$. We consider all those eigenspaces which form a strict subspace of $\ell^2(\mathbb{C})$, and thus all those eigen spaces for which the series  $\sum_{k=1}^\infty r^n z \in \mathbb{C}$ converges.
	Therefore $r$ so that $|r| < 1$ yields convergence and therefore defines the sert of acceptable eigen values. This completes the solution.
\end{itemize}


\medskip \noindent {\textbf{(3.6) } Let $V$ be a vector space over a division ring $\mathbb{F}$, let $T: V \to V$ be linear, and $p \in \mathbb{F}[x]$ be a polynomial.
\begin{itemize}
	\item If $v \in V$ is an eigenvector with eigenvalue $\lambda$ (so that $T(v) = \lambda v$) show that $[p(T)](v) = p(\lambda) v.$

	\begin{proof}
		As $p \in \mathbb{F}[x]$, then $p[x] = a_0 + a_1x + \cdots + a_nx^n$. So we evaluate 
		\begin{equation*}
			\begin{aligned}
				p[T](v) &= \left[\sum_{k=0}^n a_k T^n \right](v)	\\
				 &= \sum_{k=0}^n a_k T^n(v) = \sum_{k=0}^n a_k \lambda^n v \\
				 &=  \sum_{k=0}^n a_k [\lambda^n] v = \left( \sum_{k=0}^n a_k [\lambda]^n\right) v = p(\lambda) v.
			\end{aligned}	
		\end{equation*}
		The above follows using associativeity of $\mathbb{F}.$ This completes the proof.
	\end{proof}

	\item Let $\lambda \in \mathbb{F}$ be an eigenvalue of $T$ and suppose that $p(T) = 0$. Show that $p(\lambda) = 0$

	\begin{proof}
		If $\lambda$ is an eigenvalue of $T$ then $p(T)(v) = 0$ for all $v$, and thus $p(\lambda) v =0$ for all $v$. Since $\mathbb{F}$ is a division ring, $p(\lambda) v = 0$ for $v \neq 0$ implies that at least $p(\lambda) = 0.$
	\end{proof}


\end{itemize}



\medskip \noindent {\textbf{(3.7) } Let $V$ be a finite-dimensional vector space and let $T: V \to V$ be linear with distinct eigenvalues $\lambda_1, \dots, \lambda_m$. Show that $T$ is diagonalizable iff $V = \bigoplus_{i=1}^m Eig_{\lambda_i}.$
\begin{proof}
	Suppose $T$ is diagonalizable then by the fundamental theorem of diagonalizability (4.3.2), this is if and only if there is a basis $\scriptb$ of $V$ consisting of eigenvectors of $T$. By proposition  (4.4.34) we have that 
	\begin{equation*}
		V = \bigoplus_{b \in \scriptb} Span(b) = \bigoplus_{i=1}^m Eig_{\lambda_i, T}.
	\end{equation*}

	In the other direction suppose that $V = \bigoplus_{i=1}^m Eig_{\lambda_i}.$ Then let $\scriptb_i$ be a basis for $Eig_{\lambda_i}$. Since $V$ is a direct sum of the eigen spaces, then
	$0 = e_1 + \cdots + e_m$ implies every $e_k = 0$. Then writing $e_l$ interms of its basis in $\scriptb_k$, we yield that the set $\bigcup \scriptb_i =: \scriptb$ is linearly independent. The set also spans $V$ since $V = \sum_{i=1}^m Eig_{\lambda_i}.$ This gives $\scriptb$ a basis of $V$ and therefore by the fundaemental theorem of diagonalizability, $T$ is diagonalizable.
\end{proof}

\medskip \noindent {\textbf{(3.8) } Let $V$ be a finite-dimensional vector space over a field and let $S,T : V \to V$ be diagonalizable linear-maps. Show that $S,T$ are simultaneously  diagonalizable if and only if $S$ and $T$ commute.
\begin{proof}
	Suppose that $S,T$ are simultaneously diagonalizable with common basis $\scriptb$. Then for every $v \in V$ it follow that 
	\begin{equation*}
		\begin{aligned}
			\ [S \circ T[v]]_\scriptb &= [S \circ T]_\scriptb [v]_\scriptb \\
			&= \left[\sum_{j=1}^n S_j^p \sum_{k=1}^n T_{j}^k [v]_\scriptb^k \right]_{p=1}^n \\
			&= \left[ T_{p}^p S_p^p [v]_\scriptb^p \right]_{p=1}^n = \left[  S_p^p T_{p}^p [v]_\scriptb^p \right]_{p=1}^n \\
			 &= \left[\sum_{j=1}^n T_j^p \sum_{k=1}^n S_{j}^k [v]_\scriptb^k \right]_{p=1}^n \\
			 &= [T \circ S]_\scriptb [v]_\scriptb
		\end{aligned}
	\end{equation*}
	by the commutativity of the base field. 
\end{proof}
 \end{document}\end
